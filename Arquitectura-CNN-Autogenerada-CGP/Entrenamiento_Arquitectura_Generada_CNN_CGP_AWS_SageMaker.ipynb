{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento de arquitectura de CNN autogenerada mediante Programacion Genetica Cartesiana usando AWS SageMaker\n",
        "\n",
        "### Alumno : Gustavo Ayrton Bitocchi\n",
        "### Director : Diego Alexis Evin\n",
        "### Universidad Austral Cohorte 2020/21\n",
        "### Trabajo final de Maestria\n",
        "------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#### El siguiente notebook fue ejecutado utilizando una instancia ml.p3.2xlarge en AWS SageMaker y, previamente, guardando el conjunto de datos en un Bucket S3."
      ],
      "metadata": {
        "id": "M-uCTQvBde_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "2IurLQqZdrIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalacion de bibliotecas\n",
        "\n",
        "pip install cloudpathlib"
      ],
      "metadata": {
        "id": "MHBTkQpRdlQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importacion de bibliotecas\n",
        "\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import pickle\n",
        "import traceback\n",
        "import sys\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import boto3\n",
        "import botocore\n",
        "\n",
        "import multiprocessing.pool\n",
        "import multiprocessing as mp\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "from matplotlib.image import imread\n",
        "from collections import OrderedDict\n",
        "from cloudpathlib import CloudPath "
      ],
      "metadata": {
        "id": "0HGvtQTcdf2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga del conjunto de datos desde AWS S3"
      ],
      "metadata": {
        "id": "grxFhPN5dtsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos conjunto de entrenamiento de bucket S3\n",
        "\n",
        "cp = CloudPath(\"s3://xray-tesis-austral-bucket/Conjunto de datos/Entrenamiento/\")\n",
        "cp.download_to(\"Entrenamiento\")"
      ],
      "metadata": {
        "id": "nSwJ8ddfdwY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicion de clases que contienen implementacion de CNN generadas por CGP"
      ],
      "metadata": {
        "id": "zqdvnOHMd6aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definicion de clase de entrenamiento de CNN autogenerada\n",
        "\n",
        "class CNN_train():\n",
        "    def __init__(self, neural_network_generated, gpu_id = 0, epoch_number, batch_size, learning_rate, train_directory, number_workers, train_size_percentage):\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.neural_network_generated = neural_network_generated\n",
        "        self.gpu_id = gpu_id\n",
        "        self.epoch_number = epoch_number\n",
        "        self.train_directory = train_directory\n",
        "        self.number_workers = number_workers\n",
        "        self.train_size_percentage = train_size_percentage\n",
        "\n",
        "    def __call__(self):\n",
        "        # Obtenemos dataloaders de entrenamiento y validacion\n",
        "        train_dataloader, test_dataloader = get_traininig_validation_dataloaders(self.batch_size, self.train_directory, self.number_workers, self.train_size_percentage)\n",
        "        print('Cantidad de ejemplos de entrenamiento:', len(self.dataloader.dataset))\n",
        "        print('Arquitectura a entrenar:', self.neural_network_generated)\n",
        "        print('ID de la GPU utilizada:', self.gpu_id)\n",
        "        print('Cantidad de Epocas:', self.epoch_number)\n",
        "        print('Tamaño de Lote:', self.batch_size)\n",
        "        print('Tasa de Aprendizaje:', self.learning_rate)\n",
        "\n",
        "        torch.backends.cudnn.benchmark = True # Activamos modo Benchmark ya que el tamaño de entrada no varia, por lo cual mejoraremos el rendimiento en ejecucion\n",
        "        torch.cuda.empty_cache() # Liberamos memoria cache\n",
        "        \n",
        "        model = CGP_TO_CNN(self.neural_network_generated) # Convertimos representacion CGP a CNN\n",
        "        model.apply(self.weights_init_kaiming) # Aplicamos pesos iniciales al modelo por el metodo de He\n",
        "        model.cuda(self.gpu_id) # Movemos modelo a GPU asignada\n",
        "\n",
        "        # Definimos criterio que nos permitira evaluar el rendimiento del modelo (Aplicando pesos por las clases desbalanceadas)\n",
        "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([3901/(1349+3901), 1349/(1349+3901)]))\n",
        "        criterion.cuda(self.gpu_id) # Movemos criterio a GPU asignada\n",
        "\n",
        "        # Creamos optimizador Adam con β_1=0.9, β_2=0.999, ε=1.0 x 10^(-8)\n",
        "        optimizer = optim.Adam(model.parameters(), lr = self.learning_rate)\n",
        "\n",
        "        input = torch.FloatTensor(self.batch_size, 3, 224, 224) # Input de entrada definiendo tamaño de la imagen 224x224x3\n",
        "        input = input.cuda(self.gpu_id)\n",
        "\n",
        "        label = torch.LongTensor(self.batch_size)\n",
        "        label = label.cuda(self.gpu_id)\n",
        "\n",
        "        validation_loss_max = 0\n",
        "        model_max = copy.deepcopy(model)\n",
        "\n",
        "        # Iteramos epoca por epoca\n",
        "        for epoch in range(1, self.epoch_num+1):\n",
        "            print('Epoca', epoch)\n",
        "            start_time = time.time()\n",
        "            training_loss = 0\n",
        "            labels = []\n",
        "            predictions = []\n",
        "\n",
        "            for module in model.children():\n",
        "                module.train(True)\n",
        "\n",
        "            for _, (data, target) in enumerate(train_dataloader):\n",
        "                data = data.cuda(self.gpu_id)\n",
        "                target = target.cuda(self.gpu_id)\n",
        "\n",
        "                input.resize_as_(data).copy_(data)\n",
        "                input_ = Variable(input)\n",
        "\n",
        "                label.resize_as_(target).copy_(target)\n",
        "                label_ = Variable(label)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                try:\n",
        "                    output = model(input_, None)\n",
        "                except:\n",
        "                    traceback.print_exc()\n",
        "                    return 0.\n",
        "\n",
        "                criterion_loss = criterion(output, label_)\n",
        "                training_loss += criterion_loss.data\n",
        "                criterion_loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                label_ = label_.cpu()\n",
        "                labels.extend(label_.data.tolist())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                predicted = predicted.cpu()\n",
        "                predictions.extend(predicted.tolist())\n",
        "\n",
        "            print('Conjunto de Entrenamiento : Perdida Promedio : {:.4f}'.format(training_loss))\n",
        "            print('Conjunto de Entrenamiento : AUC-ROC Promedio : {:.4f}'.format(roc_auc_score(labels, predictions)))\n",
        "            print('Tiempo de Entrenamiento: ', time.time()-start_time)\n",
        "\n",
        "            for module in model.children():\n",
        "              module.train(False)  \n",
        "            validation_loss = self.validate_model(model, criterion, input, label, test_dataloader)\n",
        "\n",
        "            if(validation_loss >= validation_loss_max):\n",
        "              model_max = copy.deepcopy(model)\n",
        "              validation_loss_max = validation_loss\n",
        "\n",
        "        torch.save(model_max, './modelo_CNN_autogenerado_CGP.pth')\n",
        "\n",
        "        return validation_loss\n",
        "\n",
        "    def get_traininig_validation_dataloaders(self, batch_size, train_directory, number_workers, train_size_percentage):\n",
        "      random_seed = 2020\n",
        "      shuffle = True\n",
        "      pin_memory = True\n",
        "\n",
        "      # Definimos semilla para repetir resultados\n",
        "      torch.manual_seed(random_seed);\n",
        "\n",
        "      # Aplicamos diversas transformaciones al conjunto de entrenamiento para evitar el sobreajuste\n",
        "      training_dataset = ImageFolder(train_directory, \n",
        "                                     transform=tt.Compose([tt.Resize(255),\n",
        "                                                           tt.CenterCrop(224),\n",
        "                                                           tt.RandomHorizontalFlip(),\n",
        "                                                           tt.RandomRotation(10),\n",
        "                                                           tt.RandomGrayscale(),\n",
        "                                                           tt.RandomAffine(translate=(0.05,0.05), degrees=0),\n",
        "                                                           tt.ToTensor()]))\n",
        "\n",
        "      # Realizamos separacion entre conjunto de entrenamiento y validacion\n",
        "      train_size = round(len(training_dataset)*train_size_percentage)\n",
        "      val_size = len(training_dataset) - train_size\n",
        "\n",
        "      training_set, validation_set = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "      training_dataloader = DataLoader(training_set, batch_size, shuffle = shuffle, num_workers = number_workers, pin_memory = pin_memory)\n",
        "      validation_dataloader = DataLoader(validation_set, batch_size, shuffle = shuffle, num_workers = number_workers, pin_memory = pin_memory)\n",
        "\n",
        "      return (training_dataloader, validation_dataloader)\n",
        "\n",
        "    def weights_init_kaiming(self, model):\n",
        "      classname = model.__class__.__name__\n",
        "      if classname.find('Conv2d') != -1:\n",
        "        init.kaiming_normal_(model.weight.data, a=0, mode='fan_in')\n",
        "      elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(model.weight.data, a=0, mode='fan_in')\n",
        "      elif classname.find('BatchNorm2d') != -1:\n",
        "        init.uniform_(model.weight.data, 0.02, 1.0)\n",
        "        init.constant_(model.bias.data, 0.0)\n",
        "\n",
        "    def validate_model(self, model, criterion, input, label, test_dataloader):\n",
        "        validation_loss = 0\n",
        "        labels = []\n",
        "        predictions = []\n",
        "\n",
        "        for _, (data, target) in enumerate(test_dataloader):\n",
        "            data = data.cuda(self.gpu_id)\n",
        "            target = target.cuda(self.gpu_id)\n",
        "\n",
        "            input.resize_as_(data).copy_(data)\n",
        "            input_ = Variable(input)\n",
        "\n",
        "            label.resize_as_(target).copy_(target)\n",
        "            label_ = Variable(label)\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                  output = model(input_, None)\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                return 0.\n",
        "\n",
        "            criterion_loss = criterion(output, label_)\n",
        "            validation_loss += criterion_loss.data\n",
        "\n",
        "            label_ = label_.cpu()\n",
        "            labels.extend(label_.data.tolist())\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            predicted = predicted.cpu()\n",
        "            predictions.extend(predicted.tolist())\n",
        "\n",
        "        validation_roc_auc_score = roc_auc_score(labels, predictions)\n",
        "        print('Conjunto de Validacion : Perdida Promedio : {:.4f}'.format(validation_loss))\n",
        "        print('Conjunto de Validacion : AUC-ROC Promedio : {:.4f}'.format(validation_roc_auc_score))\n",
        "\n",
        "        return validation_roc_auc_score"
      ],
      "metadata": {
        "id": "PFtbbMHTlK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase que representa bloque Convolucional     \n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel, stride):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        pad_size = kernel // 2\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        return outputs\n",
        "\n",
        "# Clase que representa bloque Residual     \n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel, stride):\n",
        "        super(ResBlock, self).__init__()\n",
        "        pad_size = kernel // 2\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv2d(out_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        x = self.conv1(inputs1)\n",
        "        in_data = [x, inputs2]\n",
        "        small_ch_id, large_ch_id = (0, 1) if in_data[0].size(1) < in_data[1].size(1) else (1, 0)\n",
        "        offset = int(in_data[large_ch_id].size()[1] - in_data[small_ch_id].size()[1])\n",
        "        if offset != 0:\n",
        "            tmp = in_data[large_ch_id].data[:, :offset, :, :]\n",
        "            tmp = Variable(tmp).clone()\n",
        "            in_data[small_ch_id] = torch.cat([in_data[small_ch_id], tmp * 0], 1)\n",
        "        out = torch.add(in_data[0], in_data[1])\n",
        "        return self.relu(out)\n",
        "\n",
        "# Clase que representa bloque Sum             \n",
        "\n",
        "class Sum(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sum, self).__init__()\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        in_data = [inputs1, inputs2]\n",
        "        if (in_data[0].size(2) - in_data[1].size(2)) != 0:\n",
        "            small_in_id, large_in_id = (0, 1) if in_data[0].size(2) < in_data[1].size(2) else (1, 0)\n",
        "            pool_num = math.floor(in_data[large_in_id].size(2) / in_data[small_in_id].size(2))\n",
        "            for _ in range(pool_num-1):\n",
        "                in_data[large_in_id] = F.max_pool2d(in_data[large_in_id], 2, 2, 0)\n",
        "        small_ch_id, large_ch_id = (0, 1) if in_data[0].size(1) < in_data[1].size(1) else (1, 0)\n",
        "        offset = int(in_data[large_ch_id].size()[1] - in_data[small_ch_id].size()[1])\n",
        "        if offset != 0:\n",
        "            tmp = in_data[large_ch_id].data[:, :offset, :, :]\n",
        "            tmp = Variable(tmp).clone()\n",
        "            in_data[small_ch_id] = torch.cat([in_data[small_ch_id], tmp * 0], 1)\n",
        "        out = torch.add(in_data[0], in_data[1])\n",
        "        return out\n",
        "\n",
        "# Clase que representa bloque Concat        \n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Concat, self).__init__()\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        in_data = [inputs1, inputs2]\n",
        "        if (in_data[0].size(2) - in_data[1].size(2)) != 0:\n",
        "            small_in_id, large_in_id = (0, 1) if in_data[0].size(2) < in_data[1].size(2) else (1, 0)\n",
        "            pool_num = math.floor(in_data[large_in_id].size(2) / in_data[small_in_id].size(2))\n",
        "            for _ in range(pool_num-1):\n",
        "                in_data[large_in_id] = F.max_pool2d(in_data[large_in_id], 2, 2, 0)\n",
        "        return torch.cat([in_data[0], in_data[1]], 1)\n",
        "\n",
        "# Clase que transforma representacion CGP a CNN\n",
        "\n",
        "class CGP_TO_CNN(nn.Module):\n",
        "    def __init__(self, cgp):\n",
        "        super(CGP_TO_CNN, self).__init__()\n",
        "        self.cgp = cgp\n",
        "        self.pool_size = 2\n",
        "        self.arch = OrderedDict()\n",
        "        self.encode = []\n",
        "        self.channel_num = [None for _ in range(500)]\n",
        "        self.size = [None for _ in range(500)]\n",
        "        self.channel_num[0] = 3 # Cantidad de canales\n",
        "        self.size[0] = 224 # Tamaño de la imagen\n",
        "        i = 0\n",
        "        for name, in1, in2 in self.cgp:\n",
        "            if name == 'input' in name:\n",
        "                i += 1\n",
        "                continue\n",
        "            elif name == 'full':\n",
        "                self.encode.append(nn.Linear(self.channel_num[in1]*self.size[in1]*self.size[in1], 2))\n",
        "            elif name == 'Max_Pool' or name == 'Avg_Pool':\n",
        "                self.channel_num[i] = self.channel_num[in1]\n",
        "                self.size[i] = int(self.size[in1] / 2)\n",
        "                key = name.split('_')\n",
        "                func = key[0]\n",
        "                if func == 'Max':\n",
        "                    self.encode.append(nn.MaxPool2d(2,2))\n",
        "                else:\n",
        "                    self.encode.append(nn.AvgPool2d(2,2))\n",
        "            elif name == 'Concat':\n",
        "                self.channel_num[i] = self.channel_num[in1] + self.channel_num[in2]\n",
        "                small_in_id, large_in_id = (in1, in2) if self.size[in1] < self.size[in2] else (in2, in1)\n",
        "                self.size[i] = self.size[small_in_id]\n",
        "                self.encode.append(Concat())\n",
        "            elif name == 'Sum':\n",
        "                small_in_id, large_in_id = (in1, in2) if self.channel_num[in1] < self.channel_num[in2] else (in2, in1)\n",
        "                self.channel_num[i] = self.channel_num[large_in_id]\n",
        "                small_in_id, large_in_id = (in1, in2) if self.size[in1] < self.size[in2] else (in2, in1)\n",
        "                self.size[i] = self.size[small_in_id]\n",
        "                self.encode.append(Sum())\n",
        "            else:\n",
        "                key = name.split('_')\n",
        "                down =     key[0]\n",
        "                func =     key[1]\n",
        "                out_size = int(key[2])\n",
        "                kernel   = int(key[3])\n",
        "                if down == 'S':\n",
        "                    if func == 'ConvBlock':\n",
        "                        self.channel_num[i] = out_size\n",
        "                        self.size[i] = self.size[in1]\n",
        "                        self.encode.append(ConvBlock(self.channel_num[in1], out_size, kernel, stride=1))\n",
        "                    else:\n",
        "                        in_data = [out_size, self.channel_num[in1]]\n",
        "                        small_in_id, large_in_id = (0, 1) if in_data[0] < in_data[1] else (1, 0)\n",
        "                        self.channel_num[i] = in_data[large_in_id]\n",
        "                        self.size[i] = self.size[in1]\n",
        "                        self.encode.append(ResBlock(self.channel_num[in1], out_size, kernel, stride=1))\n",
        "                else:\n",
        "                    sys.exit('Error')\n",
        "            i += 1\n",
        "\n",
        "        self.layer_module = nn.ModuleList(self.encode)\n",
        "        self.outputs = [None for _ in range(len(self.cgp))]\n",
        "\n",
        "    def main(self,x):\n",
        "        outputs = self.outputs\n",
        "        outputs[0] = x\n",
        "        nodeID = 1\n",
        "        for layer in self.layer_module:\n",
        "            if isinstance(layer, ConvBlock):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]])\n",
        "            elif isinstance(layer, ResBlock):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]], outputs[self.cgp[nodeID][1]])\n",
        "            elif isinstance(layer, torch.nn.modules.linear.Linear):\n",
        "                tmp = outputs[self.cgp[nodeID][1]].view(outputs[self.cgp[nodeID][1]].size(0), -1)\n",
        "                outputs[nodeID] = layer(tmp)\n",
        "            elif isinstance(layer, torch.nn.modules.pooling.MaxPool2d) or isinstance(layer, torch.nn.modules.pooling.AvgPool2d):\n",
        "                if outputs[self.cgp[nodeID][1]].size(2) > 1:\n",
        "                    outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]])\n",
        "                else:\n",
        "                    outputs[nodeID] = outputs[self.cgp[nodeID][1]]\n",
        "            elif isinstance(layer, Concat) or isinstance(layer, Sum):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]], outputs[self.cgp[nodeID][2]])\n",
        "            else:\n",
        "                sys.exit(\"Error\")\n",
        "            nodeID += 1\n",
        "        return outputs[nodeID-1]\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.main(x)"
      ],
      "metadata": {
        "id": "qYuXtEBJq5Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos clase que define la estructura de la CNN desarrollada mediante CGP\n",
        "\n",
        "class CGP_Structure_Info(object):\n",
        "    def __init__(self, rows, columns, level_back, min_active_num, max_active_num):\n",
        "        self.input_num = 1\n",
        "\n",
        "        # Tipos de bloques\n",
        "        self.func_type = ['S_ConvBlock_32_1',    'S_ConvBlock_32_3',   'S_ConvBlock_32_5',\n",
        "                          'S_ConvBlock_128_1',    'S_ConvBlock_128_3',   'S_ConvBlock_128_5',\n",
        "                          'S_ConvBlock_64_1',     'S_ConvBlock_64_3',    'S_ConvBlock_64_5',\n",
        "                          'S_ResBlock_32_1',     'S_ResBlock_32_3',    'S_ResBlock_32_5',\n",
        "                          'S_ResBlock_128_1',     'S_ResBlock_128_3',    'S_ResBlock_128_5',\n",
        "                          'S_ResBlock_64_1',      'S_ResBlock_64_3',     'S_ResBlock_64_5',\n",
        "                          'Concat', 'Sum',\n",
        "                          'Max_Pool', 'Avg_Pool']\n",
        "                          \n",
        "        self.func_in_num = [1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            2, 2,\n",
        "                            1, 1]\n",
        "\n",
        "        self.out_num = 1\n",
        "        self.out_type = ['full']\n",
        "        self.out_in_num = [1]\n",
        "\n",
        "        self.rows = rows # Filas\n",
        "        self.columns = columns # Columnas\n",
        "        self.node_num = rows * columns # Cantidad de nodos\n",
        "        self.level_back = level_back # Niveles hacia atras\n",
        "        self.min_active_num = min_active_num # Minimo nodos activos\n",
        "        self.max_active_num = max_active_num # Maximo nodos activos\n",
        "\n",
        "        self.func_type_num = len(self.func_type)\n",
        "        self.out_type_num = len(self.out_type)\n",
        "        self.max_in_num = np.max([np.max(self.func_in_num), np.max(self.out_in_num)])"
      ],
      "metadata": {
        "id": "5ZlO1O-aq8OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos clase y metodos de individuo\n",
        "\n",
        "class Individual(object):\n",
        "\n",
        "    def __init__(self, net_info):\n",
        "        self.net_info = net_info\n",
        "        self.gene = np.zeros((self.net_info.node_num + self.net_info.out_num, self.net_info.max_in_num + 1)).astype(int)\n",
        "        self.is_active = np.empty(self.net_info.node_num + self.net_info.out_num).astype(bool)\n",
        "        self.eval = None\n",
        "        self.init_gene()\n",
        "\n",
        "    def init_gene(self):\n",
        "        for n in range(self.net_info.node_num + self.net_info.out_num):\n",
        "            type_num = self.net_info.func_type_num if n < self.net_info.node_num else self.net_info.out_type_num\n",
        "            self.gene[n][0] = np.random.randint(type_num)\n",
        "            col = np.min((int(n / self.net_info.rows), self.net_info.cols))\n",
        "            max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "            min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "                if col - self.net_info.level_back >= 0 else 0\n",
        "            for i in range(self.net_info.max_in_num):\n",
        "                self.gene[n][i + 1] = min_connect_id + np.random.randint(max_connect_id - min_connect_id)\n",
        "\n",
        "        self.check_active()\n",
        "\n",
        "    def check_course_to_out(self, n):\n",
        "        if not self.is_active[n]:\n",
        "            self.is_active[n] = True\n",
        "            t = self.gene[n][0]\n",
        "            if n >= self.net_info.node_num: \n",
        "                in_num = self.net_info.out_in_num[t]\n",
        "            else: \n",
        "                in_num = self.net_info.func_in_num[t]\n",
        "\n",
        "            for i in range(in_num):\n",
        "                if self.gene[n][i+1] >= self.net_info.input_num:\n",
        "                    self.check_course_to_out(self.gene[n][i+1] - self.net_info.input_num)\n",
        "\n",
        "    def count_active_node(self):\n",
        "        return self.is_active.sum()\n",
        "\n",
        "    def check_active(self):\n",
        "        self.is_active[:] = False\n",
        "        for n in range(self.net_info.out_num):\n",
        "            self.check_course_to_out(self.net_info.node_num + n)\n",
        "\n",
        "    def active_net_list(self):\n",
        "        net_list = [[\"input\", 0, 0]]\n",
        "        active_cnt = np.arange(self.net_info.input_num + self.net_info.node_num + self.net_info.out_num)\n",
        "        active_cnt[self.net_info.input_num:] = np.cumsum(self.is_active)\n",
        "        for n, is_a in enumerate(self.is_active):\n",
        "            if is_a:\n",
        "                t = self.gene[n][0]\n",
        "                if n < self.net_info.node_num:   \n",
        "                    type_str = self.net_info.func_type[t]\n",
        "                else:    \n",
        "                    type_str = self.net_info.out_type[t]\n",
        "                connections = [active_cnt[self.gene[n][i+1]] for i in range(self.net_info.max_in_num)]\n",
        "                net_list.append([type_str] + connections)\n",
        "        return net_list"
      ],
      "metadata": {
        "id": "yGrvjTOFnsEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos clase CGP y metodos para cargar los datos de la arquitectura autogenerada\n",
        "\n",
        "class CGP(object):\n",
        "    def __init__(self, net_info, cgp_data):\n",
        "        self.pop = [Individual(net_info) for _ in range(1)]\n",
        "        self.num_gen = 0\n",
        "        self.num_eval = 0\n",
        "        self.load_log(list(cgp_data.tail(1).values.flatten().astype(int)))\n",
        "        self.log_data(net_info_type='active_only', start_time=0)\n",
        "\n",
        "    def log_data(self, net_info_type='active_only', start_time=0):\n",
        "        log_list = [self.num_gen, self.num_eval, time.time()-start_time, self.pop[0].eval, self.pop[0].count_active_node()]\n",
        "        if net_info_type == 'active_only':\n",
        "            log_list.append(self.pop[0].active_net_list())\n",
        "        elif net_info_type == 'full':\n",
        "            log_list += self.pop[0].gene.flatten().tolist()\n",
        "        else:\n",
        "            pass\n",
        "        return log_list\n",
        "\n",
        "    def load_log(self, log_data_info):\n",
        "        self.num_gen = log_data_info[0]\n",
        "        self.num_eval = log_data_info[1]\n",
        "        net_info = self.pop[0].net_info\n",
        "        self.pop[0].eval = log_data_info[3]\n",
        "        self.pop[0].gene = np.array(log_data_info[5:]).reshape((net_info.node_num + net_info.out_num, net_info.max_in_num + 1))\n",
        "        self.pop[0].check_active()"
      ],
      "metadata": {
        "id": "-enVKVzwmZGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento de la CNN autogenerada mediante CGP"
      ],
      "metadata": {
        "id": "jwBE9IDYrWn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos estructura interna de CGP\n",
        "with open('network_cgp_info.pickle', mode='rb') as f:\n",
        "  network_cgp_info = pickle.load(f)\n",
        "\n",
        "# Cargamos log que contiene todo el entrenamiento de la generacion de la CNN mediante CGP\n",
        "cgp_data = pd.read_csv('log_cgp.txt', header=None)\n",
        "\n",
        "# Instanciamos arquitectura autogenerada encodificada como CGP (Unicamente los nodos activos)\n",
        "cnn_generated_cgp = CGP(net_info = network_cgp_info, cgp_data = cgp_data).pop[0].active_net_list()\n",
        "\n",
        "training_start_time = time.time()\n",
        "\n",
        "# Realizamos entrenamiento de la arquitectura autogenerada mediante CGP\n",
        "CNN_train(neural_network_generated = cnn_generated_cgp, \n",
        "          epoch_number = 20, \n",
        "          batch_size = 16, \n",
        "          learning_rate = 0.005, \n",
        "          number_workers = 4, \n",
        "          train_size_percentage = 0.7,\n",
        "          train_directory = 'Entrenamiento/')\n",
        "\n",
        "training_end_time = time.time()"
      ],
      "metadata": {
        "id": "3hVaELtGzhG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos tiempo de entrenamiento\n",
        "print(\"Tiempo de entrenamiento en segundos:\", end - start)"
      ],
      "metadata": {
        "id": "bSvmAT0oz5Eg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}