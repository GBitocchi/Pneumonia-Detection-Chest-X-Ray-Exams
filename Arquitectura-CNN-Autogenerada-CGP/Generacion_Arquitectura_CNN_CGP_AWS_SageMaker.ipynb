{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generación automatica de arquitectura de CNN mediante Programación Genética Cartesiana\n",
        "\n",
        "### Alumno : Gustavo Ayrton Bitocchi\n",
        "### Director : Diego Alexis Evin\n",
        "### Universidad Austral Cohorte 2020/21\n",
        "### Trabajo final de Maestria\n",
        "------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#### El siguiente notebook fue ejecutado durante 10 dias utilizando una instancia ml.p3.2xlarge en AWS SageMaker y, previamente, guardando el conjunto de datos en un Bucket S3."
      ],
      "metadata": {
        "id": "X1tdrcRxDARb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "DDMkAflfcS2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalacion de bibliotecas\n",
        "\n",
        "pip install cloudpathlib"
      ],
      "metadata": {
        "id": "E2fpsz-uDGC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bt8G-N6CozQ"
      },
      "outputs": [],
      "source": [
        "# Importacion de bibliotecas\n",
        "\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import pickle\n",
        "import traceback\n",
        "import sys\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import boto3\n",
        "import botocore\n",
        "\n",
        "import multiprocessing.pool\n",
        "import multiprocessing as mp\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as tt\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "from matplotlib.image import imread\n",
        "from collections import OrderedDict\n",
        "from cloudpathlib import CloudPath "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga del conjunto de datos desde AWS S3"
      ],
      "metadata": {
        "id": "pa94uobLcVLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos conjunto de entrenamiento de bucket S3\n",
        "\n",
        "cp = CloudPath(\"s3://xray-tesis-austral-bucket/Conjunto de datos/Entrenamiento/\")\n",
        "cp.download_to(\"Entrenamiento\")"
      ],
      "metadata": {
        "id": "hkuMoI5TKvp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicion de clases que contienen implementacion de CNN generadas por CGP"
      ],
      "metadata": {
        "id": "kDPte5TXcbL8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYbNW0-Y30fj"
      },
      "outputs": [],
      "source": [
        "# Definicion de clase de entrenamiento de CNN autogenerada\n",
        "\n",
        "class CNN_train():\n",
        "    def __init__(self, neural_network_generated, epoch_number, batch_size, learning_rate, train_directory, number_workers, train_size_percentage):\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.neural_network_generated = neural_network_generated\n",
        "        self.gpu_id = 0\n",
        "        self.epoch_number = epoch_number\n",
        "        self.train_directory = train_directory\n",
        "        self.number_workers = number_workers\n",
        "        self.train_size_percentage = train_size_percentage\n",
        "\n",
        "    def __call__(self):\n",
        "        # Obtenemos dataloaders de entrenamiento y validacion\n",
        "        train_dataloader, test_dataloader = get_traininig_validation_dataloaders(self.batch_size, self.train_directory, self.number_workers, self.train_size_percentage)\n",
        "        print('Cantidad de ejemplos de entrenamiento:', len(train_dataloader))\n",
        "        print('Arquitectura a entrenar:', self.neural_network_generated)\n",
        "        print('ID de la GPU utilizada:', self.gpu_id)\n",
        "        print('Cantidad de Epocas:', self.epoch_number)\n",
        "        print('Tamaño de Lote:', self.batch_size)\n",
        "        print('Tasa de Aprendizaje:', self.learning_rate)\n",
        "\n",
        "        torch.backends.cudnn.benchmark = True # Activamos modo Benchmark ya que el tamaño de entrada no varia, por lo cual mejoraremos el rendimiento en ejecucion\n",
        "        torch.cuda.empty_cache() # Liberamos memoria cache\n",
        "        \n",
        "        model = CGP_TO_CNN(self.neural_network_generated) # Convertimos representacion CGP a CNN\n",
        "        model.apply(self.weights_init_kaiming) # Aplicamos pesos iniciales al modelo por el metodo de He\n",
        "        model.cuda(self.gpu_id) # Movemos modelo a GPU asignada\n",
        "\n",
        "        # Definimos criterio que nos permitira evaluar el rendimiento del modelo (Aplicando pesos por las clases desbalanceadas)\n",
        "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([3901/(1349+3901), 1349/(1349+3901)]))\n",
        "        criterion.cuda(self.gpu_id) # Movemos criterio a GPU asignada\n",
        "\n",
        "        # Creamos optimizador Adam con β_1=0.9, β_2=0.999, ε=1.0 x 10^(-8)\n",
        "        optimizer = optim.Adam(model.parameters(), lr = self.learning_rate)\n",
        "\n",
        "        input = torch.FloatTensor(self.batch_size, 3, 224, 224) # Input de entrada definiendo tamaño de la imagen 224x224x3\n",
        "        input = input.cuda(self.gpu_id)\n",
        "\n",
        "        label = torch.LongTensor(self.batch_size)\n",
        "        label = label.cuda(self.gpu_id)\n",
        "\n",
        "        # Iteramos epoca por epoca\n",
        "        for epoch in range(1, self.epoch_number+1):\n",
        "            print('Epoca', epoch)\n",
        "            start_time = time.time()\n",
        "            training_loss = 0\n",
        "            labels = []\n",
        "            predictions = []\n",
        "\n",
        "            for module in model.children():\n",
        "                module.train(True)\n",
        "\n",
        "            for _, (data, target) in enumerate(train_dataloader):\n",
        "                data = data.cuda(self.gpu_id)\n",
        "                target = target.cuda(self.gpu_id)\n",
        "\n",
        "                input.resize_as_(data).copy_(data)\n",
        "                input_ = Variable(input)\n",
        "\n",
        "                label.resize_as_(target).copy_(target)\n",
        "                label_ = Variable(label)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                try:\n",
        "                    output = model(input_, None)\n",
        "                except:\n",
        "                    traceback.print_exc()\n",
        "                    return 0.\n",
        "\n",
        "                criterion_loss = criterion(output, label_)\n",
        "                training_loss += criterion_loss.data\n",
        "                criterion_loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                label_ = label_.cpu()\n",
        "                labels.extend(label_.data.tolist())\n",
        "\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                predicted = predicted.cpu()\n",
        "                predictions.extend(predicted.tolist())\n",
        "\n",
        "            print('Conjunto de Entrenamiento : Perdida Promedio : {:.4f}'.format(training_loss))\n",
        "            print('Conjunto de Entrenamiento : AUC-ROC Promedio : {:.4f}'.format(roc_auc_score(labels, predictions)))\n",
        "            print('Tiempo de Entrenamiento: ', time.time()-start_time)\n",
        "\n",
        "            if epoch == self.epoch_num:\n",
        "              for module in model.children():\n",
        "                module.train(False)  \n",
        "              validation_loss = self.validate_model(model, criterion, input, label, test_dataloader)\n",
        "\n",
        "        torch.save(model.state_dict(), './modelo_%d.pth' % int(self.gpu_id))\n",
        "\n",
        "        return validation_loss\n",
        "\n",
        "    def get_traininig_validation_dataloaders(self, batch_size, train_directory, number_workers, train_size_percentage):\n",
        "      random_seed = 2020\n",
        "      shuffle = True\n",
        "      pin_memory = True\n",
        "\n",
        "      # Definimos semilla para repetir resultados\n",
        "      torch.manual_seed(random_seed);\n",
        "\n",
        "      # Aplicamos diversas transformaciones al conjunto de entrenamiento para evitar el sobreajuste\n",
        "      training_dataset = ImageFolder(train_directory, \n",
        "                                     transform=tt.Compose([tt.Resize(255),\n",
        "                                                           tt.CenterCrop(224),\n",
        "                                                           tt.RandomHorizontalFlip(),\n",
        "                                                           tt.RandomRotation(10),\n",
        "                                                           tt.RandomGrayscale(),\n",
        "                                                           tt.RandomAffine(translate=(0.05,0.05), degrees=0),\n",
        "                                                           tt.ToTensor()]))\n",
        "\n",
        "      # Realizamos separacion entre conjunto de entrenamiento y validacion\n",
        "      train_size = round(len(training_dataset)*train_size_percentage)\n",
        "      val_size = len(training_dataset) - train_size\n",
        "\n",
        "      training_set, validation_set = random_split(training_dataset, [train_size, val_size])\n",
        "\n",
        "      training_dataloader = DataLoader(training_set, batch_size, shuffle = shuffle, num_workers = number_workers, pin_memory = pin_memory)\n",
        "      validation_dataloader = DataLoader(validation_set, batch_size, shuffle = shuffle, num_workers = number_workers, pin_memory = pin_memory)\n",
        "\n",
        "      return (training_dataloader, validation_dataloader)\n",
        "\n",
        "    def weights_init_kaiming(self, model):\n",
        "      classname = model.__class__.__name__\n",
        "      if classname.find('Conv2d') != -1:\n",
        "        init.kaiming_normal_(model.weight.data, a=0, mode='fan_in')\n",
        "      elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(model.weight.data, a=0, mode='fan_in')\n",
        "      elif classname.find('BatchNorm2d') != -1:\n",
        "        init.uniform_(model.weight.data, 0.02, 1.0)\n",
        "        init.constant_(model.bias.data, 0.0)\n",
        "\n",
        "    def validate_model(self, model, criterion, input, label, test_dataloader):\n",
        "        validation_loss = 0\n",
        "        labels = []\n",
        "        predictions = []\n",
        "\n",
        "        for _, (data, target) in enumerate(test_dataloader):\n",
        "            data = data.cuda(self.gpu_id)\n",
        "            target = target.cuda(self.gpu_id)\n",
        "\n",
        "            input.resize_as_(data).copy_(data)\n",
        "            input_ = Variable(input)\n",
        "\n",
        "            label.resize_as_(target).copy_(target)\n",
        "            label_ = Variable(label)\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                  output = model(input_, None)\n",
        "            except:\n",
        "                traceback.print_exc()\n",
        "                return 0.\n",
        "\n",
        "            criterion_loss = criterion(output, label_)\n",
        "            validation_loss += criterion_loss.data\n",
        "\n",
        "            label_ = label_.cpu()\n",
        "            labels.extend(label_.data.tolist())\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            predicted = predicted.cpu()\n",
        "            predictions.extend(predicted.tolist())\n",
        "\n",
        "        validation_roc_auc_score = roc_auc_score(labels, predictions)\n",
        "        print('Conjunto de Validacion : Perdida Promedio : {:.4f}'.format(validation_loss))\n",
        "        print('Conjunto de Validacion : AUC-ROC Promedio : {:.4f}'.format(validation_roc_auc_score))\n",
        "\n",
        "        return validation_roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbYxGkWG7E5_"
      },
      "outputs": [],
      "source": [
        "# Clase que representa bloque Convolucional     \n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel, stride):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        pad_size = kernel // 2\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size),\n",
        "                                       nn.ReLU(inplace=True),)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.conv1(inputs)\n",
        "        return outputs\n",
        "\n",
        "# Clase que representa bloque Residual     \n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_size, out_size, kernel, stride):\n",
        "        super(ResBlock, self).__init__()\n",
        "        pad_size = kernel // 2\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size),\n",
        "                                       nn.ReLU(inplace=True),\n",
        "                                       nn.Conv2d(out_size, out_size, kernel, stride=stride, padding=pad_size, bias=False),\n",
        "                                       nn.BatchNorm2d(out_size))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        x = self.conv1(inputs1)\n",
        "        in_data = [x, inputs2]\n",
        "        small_ch_id, large_ch_id = (0, 1) if in_data[0].size(1) < in_data[1].size(1) else (1, 0)\n",
        "        offset = int(in_data[large_ch_id].size()[1] - in_data[small_ch_id].size()[1])\n",
        "        if offset != 0:\n",
        "            tmp = in_data[large_ch_id].data[:, :offset, :, :]\n",
        "            tmp = Variable(tmp).clone()\n",
        "            in_data[small_ch_id] = torch.cat([in_data[small_ch_id], tmp * 0], 1)\n",
        "        out = torch.add(in_data[0], in_data[1])\n",
        "        return self.relu(out)\n",
        "\n",
        "# Clase que representa bloque Sum             \n",
        "\n",
        "class Sum(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sum, self).__init__()\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        in_data = [inputs1, inputs2]\n",
        "        if (in_data[0].size(2) - in_data[1].size(2)) != 0:\n",
        "            small_in_id, large_in_id = (0, 1) if in_data[0].size(2) < in_data[1].size(2) else (1, 0)\n",
        "            pool_num = math.floor(in_data[large_in_id].size(2) / in_data[small_in_id].size(2))\n",
        "            for _ in range(pool_num-1):\n",
        "                in_data[large_in_id] = F.max_pool2d(in_data[large_in_id], 2, 2, 0)\n",
        "        small_ch_id, large_ch_id = (0, 1) if in_data[0].size(1) < in_data[1].size(1) else (1, 0)\n",
        "        offset = int(in_data[large_ch_id].size()[1] - in_data[small_ch_id].size()[1])\n",
        "        if offset != 0:\n",
        "            tmp = in_data[large_ch_id].data[:, :offset, :, :]\n",
        "            tmp = Variable(tmp).clone()\n",
        "            in_data[small_ch_id] = torch.cat([in_data[small_ch_id], tmp * 0], 1)\n",
        "        out = torch.add(in_data[0], in_data[1])\n",
        "        return out\n",
        "\n",
        "# Clase que representa bloque Concat        \n",
        "\n",
        "class Concat(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Concat, self).__init__()\n",
        "\n",
        "    def forward(self, inputs1, inputs2):\n",
        "        in_data = [inputs1, inputs2]\n",
        "        if (in_data[0].size(2) - in_data[1].size(2)) != 0:\n",
        "            small_in_id, large_in_id = (0, 1) if in_data[0].size(2) < in_data[1].size(2) else (1, 0)\n",
        "            pool_num = math.floor(in_data[large_in_id].size(2) / in_data[small_in_id].size(2))\n",
        "            for _ in range(pool_num-1):\n",
        "                in_data[large_in_id] = F.max_pool2d(in_data[large_in_id], 2, 2, 0)\n",
        "        return torch.cat([in_data[0], in_data[1]], 1)\n",
        "\n",
        "# Clase que transforma representacion CGP a CNN\n",
        "\n",
        "class CGP_TO_CNN(nn.Module):\n",
        "    def __init__(self, cgp):\n",
        "        super(CGP_TO_CNN, self).__init__()\n",
        "        self.cgp = cgp\n",
        "        self.pool_size = 2\n",
        "        self.arch = OrderedDict()\n",
        "        self.encode = []\n",
        "        self.channel_num = [None for _ in range(500)]\n",
        "        self.size = [None for _ in range(500)]\n",
        "        self.channel_num[0] = 3 # Cantidad de canales\n",
        "        self.size[0] = 224 # Tamaño de la imagen\n",
        "        i = 0\n",
        "        for name, in1, in2 in self.cgp:\n",
        "            if name == 'input' in name:\n",
        "                i += 1\n",
        "                continue\n",
        "            elif name == 'full':\n",
        "                self.encode.append(nn.Linear(self.channel_num[in1]*self.size[in1]*self.size[in1], 2))\n",
        "            elif name == 'Max_Pool' or name == 'Avg_Pool':\n",
        "                self.channel_num[i] = self.channel_num[in1]\n",
        "                self.size[i] = int(self.size[in1] / 2)\n",
        "                key = name.split('_')\n",
        "                func = key[0]\n",
        "                if func == 'Max':\n",
        "                    self.encode.append(nn.MaxPool2d(2,2))\n",
        "                else:\n",
        "                    self.encode.append(nn.AvgPool2d(2,2))\n",
        "            elif name == 'Concat':\n",
        "                self.channel_num[i] = self.channel_num[in1] + self.channel_num[in2]\n",
        "                small_in_id, large_in_id = (in1, in2) if self.size[in1] < self.size[in2] else (in2, in1)\n",
        "                self.size[i] = self.size[small_in_id]\n",
        "                self.encode.append(Concat())\n",
        "            elif name == 'Sum':\n",
        "                small_in_id, large_in_id = (in1, in2) if self.channel_num[in1] < self.channel_num[in2] else (in2, in1)\n",
        "                self.channel_num[i] = self.channel_num[large_in_id]\n",
        "                small_in_id, large_in_id = (in1, in2) if self.size[in1] < self.size[in2] else (in2, in1)\n",
        "                self.size[i] = self.size[small_in_id]\n",
        "                self.encode.append(Sum())\n",
        "            else:\n",
        "                key = name.split('_')\n",
        "                down =     key[0]\n",
        "                func =     key[1]\n",
        "                out_size = int(key[2])\n",
        "                kernel   = int(key[3])\n",
        "                if down == 'S':\n",
        "                    if func == 'ConvBlock':\n",
        "                        self.channel_num[i] = out_size\n",
        "                        self.size[i] = self.size[in1]\n",
        "                        self.encode.append(ConvBlock(self.channel_num[in1], out_size, kernel, stride=1))\n",
        "                    else:\n",
        "                        in_data = [out_size, self.channel_num[in1]]\n",
        "                        small_in_id, large_in_id = (0, 1) if in_data[0] < in_data[1] else (1, 0)\n",
        "                        self.channel_num[i] = in_data[large_in_id]\n",
        "                        self.size[i] = self.size[in1]\n",
        "                        self.encode.append(ResBlock(self.channel_num[in1], out_size, kernel, stride=1))\n",
        "                else:\n",
        "                    sys.exit('Error')\n",
        "            i += 1\n",
        "\n",
        "        self.layer_module = nn.ModuleList(self.encode)\n",
        "        self.outputs = [None for _ in range(len(self.cgp))]\n",
        "\n",
        "    def main(self,x):\n",
        "        outputs = self.outputs\n",
        "        outputs[0] = x\n",
        "        nodeID = 1\n",
        "        for layer in self.layer_module:\n",
        "            if isinstance(layer, ConvBlock):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]])\n",
        "            elif isinstance(layer, ResBlock):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]], outputs[self.cgp[nodeID][1]])\n",
        "            elif isinstance(layer, torch.nn.modules.linear.Linear):\n",
        "                tmp = outputs[self.cgp[nodeID][1]].view(outputs[self.cgp[nodeID][1]].size(0), -1)\n",
        "                outputs[nodeID] = layer(tmp)\n",
        "            elif isinstance(layer, torch.nn.modules.pooling.MaxPool2d) or isinstance(layer, torch.nn.modules.pooling.AvgPool2d):\n",
        "                if outputs[self.cgp[nodeID][1]].size(2) > 1:\n",
        "                    outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]])\n",
        "                else:\n",
        "                    outputs[nodeID] = outputs[self.cgp[nodeID][1]]\n",
        "            elif isinstance(layer, Concat) or isinstance(layer, Sum):\n",
        "                outputs[nodeID] = layer(outputs[self.cgp[nodeID][1]], outputs[self.cgp[nodeID][2]])\n",
        "            else:\n",
        "                sys.exit(\"Error\")\n",
        "            nodeID += 1\n",
        "        return outputs[nodeID-1]\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-d_z87r0Wsc"
      },
      "outputs": [],
      "source": [
        "# Creamos clase que define la estructura de la CNN desarrollada mediante CGP\n",
        "\n",
        "class CGP_Structure_Info(object):\n",
        "    def __init__(self, rows, columns, level_back, min_active_num, max_active_num):\n",
        "        self.input_num = 1\n",
        "\n",
        "        # Tipos de bloques\n",
        "        self.func_type = ['S_ConvBlock_32_1',    'S_ConvBlock_32_3',   'S_ConvBlock_32_5',\n",
        "                          'S_ConvBlock_128_1',    'S_ConvBlock_128_3',   'S_ConvBlock_128_5',\n",
        "                          'S_ConvBlock_64_1',     'S_ConvBlock_64_3',    'S_ConvBlock_64_5',\n",
        "                          'S_ResBlock_32_1',     'S_ResBlock_32_3',    'S_ResBlock_32_5',\n",
        "                          'S_ResBlock_128_1',     'S_ResBlock_128_3',    'S_ResBlock_128_5',\n",
        "                          'S_ResBlock_64_1',      'S_ResBlock_64_3',     'S_ResBlock_64_5',\n",
        "                          'Concat', 'Sum',\n",
        "                          'Max_Pool', 'Avg_Pool']\n",
        "                          \n",
        "        self.func_in_num = [1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            1, 1, 1,\n",
        "                            2, 2,\n",
        "                            1, 1]\n",
        "\n",
        "        self.out_num = 1\n",
        "        self.out_type = ['full']\n",
        "        self.out_in_num = [1]\n",
        "\n",
        "        self.rows = rows # Filas\n",
        "        self.columns = columns # Columnas\n",
        "        self.node_num = rows * columns # Cantidad de nodos\n",
        "        self.level_back = level_back # Niveles hacia atras\n",
        "        self.min_active_num = min_active_num # Minimo nodos activos\n",
        "        self.max_active_num = max_active_num # Maximo nodos activos\n",
        "\n",
        "        self.func_type_num = len(self.func_type)\n",
        "        self.out_type_num = len(self.out_type)\n",
        "        self.max_in_num = np.max([np.max(self.func_in_num), np.max(self.out_in_num)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBSNeLNR1dgc"
      },
      "outputs": [],
      "source": [
        "# Creamos Daemon Processes para paralelizar la generacion de la arquitectura (si se cuenta con GPUs adicionales)\n",
        "\n",
        "class NoDaemonProcess(mp.Process):\n",
        "    def _get_daemon(self):\n",
        "        return False\n",
        "    def _set_daemon(self, value):\n",
        "        pass\n",
        "    daemon = property(_get_daemon, _set_daemon)\n",
        "\n",
        "class NoDaemonProcessPool(multiprocessing.pool.Pool):\n",
        "    Process = NoDaemonProcess\n",
        "\n",
        "class CNN_Generated_Evaluation(object):\n",
        "    def __init__(self, gpu_number, epoch_number, batch_size, learning_rate, number_workers, train_size_percentage, train_directory):\n",
        "        self.gpu_number = gpu_number\n",
        "        self.epoch_number = epoch_number\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.number_workers = number_workers\n",
        "        self.train_size_percentage = train_size_percentage\n",
        "        self.train_directory = train_directory\n",
        "\n",
        "    def __call__(self, neural_network_lists):\n",
        "        neural_network_lists_length = len(neural_network_lists)\n",
        "        evaluations = np.zeros(neural_network_lists_length)\n",
        "\n",
        "        for i in np.arange(0, neural_network_lists_length, self.gpu_number):\n",
        "            process_number = np.min((i + self.gpu_number, neural_network_lists_length)) - i # Creamos x cantidad de procesos para paralelizar la ejecucion (x es el minimo entre la cantidad de GPUs y la cantidad de CNNs generadas)\n",
        "            pool = NoDaemonProcessPool(process_number)\n",
        "            arg_data = [(self.cnn_generated_eval, neural_network_lists[i+j], j) for j in range(process_number)]\n",
        "            evaluations[i:i+process_number] = pool.map(self.arg_wrapper_mp, arg_data)\n",
        "            pool.terminate()\n",
        "\n",
        "        return evaluations\n",
        "\n",
        "    def arg_wrapper_mp(self, args):\n",
        "      return args[0](*args[1:])\n",
        "      \n",
        "    def cnn_generated_eval(self, neural_network_generated, gpu_id):\n",
        "      train = CNN_train(neural_network_generated, gpu_id, self.epoch_number, self.batch_size, self.learning_rate, self.train_directory, self.number_workers, self.train_size_percentage)\n",
        "      evaluation_with_neural_network_generated = train()\n",
        "\n",
        "      return evaluation_with_neural_network_generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdVujVLF2tII"
      },
      "outputs": [],
      "source": [
        "# Definimos clase y metodos de individuo\n",
        "\n",
        "class Individual(object):\n",
        "    def __init__(self, net_info):\n",
        "        self.net_info = net_info\n",
        "        self.gene = np.zeros((self.net_info.node_num + self.net_info.out_num, self.net_info.max_in_num + 1)).astype(int)\n",
        "        self.is_active = np.empty(self.net_info.node_num + self.net_info.out_num).astype(bool)\n",
        "        self.is_pool = np.empty(self.net_info.node_num + self.net_info.out_num).astype(bool)\n",
        "        self.init_gene_with_conv()\n",
        "\n",
        "    def init_gene_with_conv(self):\n",
        "        arch = ['S_ConvBlock_64_3']\n",
        "       \n",
        "        input_layer_num = int(self.net_info.input_num / self.net_info.rows) + 1\n",
        "        output_layer_num = int(self.net_info.out_num / self.net_info.rows) + 1\n",
        "        layer_ids = [((self.net_info.columns - 1 - input_layer_num - output_layer_num) + i) // (len(arch)) for i in range(len(arch))]\n",
        "        prev_id = 0\n",
        "        current_layer = input_layer_num\n",
        "        block_ids = []\n",
        "        \n",
        "        for i, idx in enumerate(layer_ids):\n",
        "            \n",
        "            current_layer += idx\n",
        "            n = current_layer * self.net_info.rows + np.random.randint(self.net_info.rows)\n",
        "            block_ids.append(n)\n",
        "            self.gene[n][0] = self.net_info.func_type.index(arch[i])\n",
        "            col = np.min((int(n / self.net_info.rows), self.net_info.columns))\n",
        "            max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "            min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "                if col - self.net_info.level_back >= 0 else 0\n",
        "            \n",
        "            self.gene[n][1] = prev_id\n",
        "            for j in range(1, self.net_info.max_in_num):\n",
        "                self.gene[n][j + 1] = min_connect_id + np.random.randint(max_connect_id - min_connect_id)\n",
        "            \n",
        "            prev_id = n + self.net_info.input_num\n",
        "            \n",
        "        n = self.net_info.node_num\n",
        "        type_num = self.net_info.func_type_num if n < self.net_info.node_num else self.net_info.out_type_num\n",
        "        self.gene[n][0] = np.random.randint(type_num)\n",
        "        col = np.min((int(n / self.net_info.rows), self.net_info.columns))\n",
        "        max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "        min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "            if col - self.net_info.level_back >= 0 else 0\n",
        "        \n",
        "        self.gene[n][1] = prev_id\n",
        "        for i in range(1, self.net_info.max_in_num):\n",
        "            self.gene[n][i + 1] = min_connect_id + np.random.randint(max_connect_id - min_connect_id)        \n",
        "        block_ids.append(n) \n",
        "           \n",
        "        for n in range(self.net_info.node_num + self.net_info.out_num):\n",
        "            \n",
        "            if n in block_ids:\n",
        "                continue\n",
        "            \n",
        "            type_num = self.net_info.func_type_num if n < self.net_info.node_num else self.net_info.out_type_num\n",
        "            self.gene[n][0] = np.random.randint(type_num)\n",
        "            col = np.min((int(n / self.net_info.rows), self.net_info.columns))\n",
        "            max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "            min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "                if col - self.net_info.level_back >= 0 else 0\n",
        "            for i in range(self.net_info.max_in_num):\n",
        "                self.gene[n][i + 1] = min_connect_id + np.random.randint(max_connect_id - min_connect_id)\n",
        "\n",
        "        self.check_active()\n",
        "\n",
        "    def __check_course_to_out(self, n):\n",
        "        if not self.is_active[n]:\n",
        "            self.is_active[n] = True\n",
        "            t = self.gene[n][0]\n",
        "            if n >= self.net_info.node_num:\n",
        "                in_num = self.net_info.out_in_num[t]\n",
        "            else:\n",
        "                in_num = self.net_info.func_in_num[t]\n",
        "\n",
        "            for i in range(in_num):\n",
        "                if self.gene[n][i+1] >= self.net_info.input_num:\n",
        "                    self.__check_course_to_out(self.gene[n][i+1] - self.net_info.input_num)\n",
        "\n",
        "    def check_active(self):\n",
        "        self.is_active[:] = False\n",
        "        for n in range(self.net_info.out_num):\n",
        "            self.__check_course_to_out(self.net_info.node_num + n)\n",
        "    \n",
        "    def check_pool(self):\n",
        "        is_pool = True\n",
        "        pool_num = 0\n",
        "        for n in range(self.net_info.node_num + self.net_info.out_num):\n",
        "            if self.is_active[n]:\n",
        "                if self.gene[n][0] > 19:\n",
        "                    is_pool = False\n",
        "                    pool_num += 1\n",
        "        return is_pool, pool_num\n",
        "\n",
        "    def __mutate(self, current, min_int, max_int):\n",
        "        mutated_gene = current\n",
        "        while current == mutated_gene:\n",
        "            mutated_gene = min_int + np.random.randint(max_int - min_int)\n",
        "        return mutated_gene\n",
        "\n",
        "    def mutation(self, mutation_rate):\n",
        "        active_check = False\n",
        "\n",
        "        for n in range(self.net_info.node_num + self.net_info.out_num):\n",
        "            t = self.gene[n][0]\n",
        "            type_num = self.net_info.func_type_num if n < self.net_info.node_num else self.net_info.out_type_num\n",
        "            if np.random.rand() < mutation_rate and type_num > 1:\n",
        "                self.gene[n][0] = self.__mutate(self.gene[n][0], 0, type_num)\n",
        "                if self.is_active[n]:\n",
        "                    active_check = True\n",
        "            col = np.min((int(n / self.net_info.rows), self.net_info.columns))\n",
        "            max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "            min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "                if col - self.net_info.level_back >= 0 else 0\n",
        "            in_num = self.net_info.func_in_num[t] if n < self.net_info.node_num else self.net_info.out_in_num[t]\n",
        "            for i in range(self.net_info.max_in_num):\n",
        "                if np.random.rand() < mutation_rate and max_connect_id - min_connect_id > 1:\n",
        "                    self.gene[n][i+1] = self.__mutate(self.gene[n][i+1], min_connect_id, max_connect_id)\n",
        "                    if self.is_active[n] and i < in_num:\n",
        "                        active_check = True\n",
        "\n",
        "        self.check_active()\n",
        "        return active_check\n",
        "\n",
        "    def neutral_mutation(self, mutation_rate):\n",
        "        for n in range(self.net_info.node_num + self.net_info.out_num):\n",
        "            t = self.gene[n][0]\n",
        "            type_num = self.net_info.func_type_num if n < self.net_info.node_num else self.net_info.out_type_num\n",
        "            if not self.is_active[n] and np.random.rand() < mutation_rate and type_num > 1:\n",
        "                self.gene[n][0] = self.__mutate(self.gene[n][0], 0, type_num)\n",
        "            col = np.min((int(n / self.net_info.rows), self.net_info.columns))\n",
        "            max_connect_id = col * self.net_info.rows + self.net_info.input_num\n",
        "            min_connect_id = (col - self.net_info.level_back) * self.net_info.rows + self.net_info.input_num \\\n",
        "                if col - self.net_info.level_back >= 0 else 0\n",
        "            in_num = self.net_info.func_in_num[t] if n < self.net_info.node_num else self.net_info.out_in_num[t]\n",
        "            for i in range(self.net_info.max_in_num):\n",
        "                if (not self.is_active[n] or i >= in_num) and np.random.rand() < mutation_rate \\\n",
        "                        and max_connect_id - min_connect_id > 1:\n",
        "                    self.gene[n][i+1] = self.__mutate(self.gene[n][i+1], min_connect_id, max_connect_id)\n",
        "\n",
        "        self.check_active()\n",
        "        return False\n",
        "\n",
        "    def count_active_node(self):\n",
        "        return self.is_active.sum()\n",
        "\n",
        "    def copy(self, source):\n",
        "        self.net_info = source.net_info\n",
        "        self.gene = source.gene.copy()\n",
        "        self.is_active = source.is_active.copy()\n",
        "        self.eval = source.eval\n",
        "\n",
        "    def active_net_list(self):\n",
        "        net_list = [[\"input\", 0, 0]]\n",
        "        active_cnt = np.arange(self.net_info.input_num + self.net_info.node_num + self.net_info.out_num)\n",
        "        active_cnt[self.net_info.input_num:] = np.cumsum(self.is_active)\n",
        "\n",
        "        for n, is_a in enumerate(self.is_active):\n",
        "            if is_a:\n",
        "                t = self.gene[n][0]\n",
        "                if n < self.net_info.node_num:\n",
        "                    type_str = self.net_info.func_type[t]\n",
        "                else: \n",
        "                    type_str = self.net_info.out_type[t]\n",
        "\n",
        "                connections = [active_cnt[self.gene[n][i+1]] for i in range(self.net_info.max_in_num)]\n",
        "                net_list.append([type_str] + connections)\n",
        "        return net_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSdGpgFa2Kf_"
      },
      "outputs": [],
      "source": [
        "# Definimos clase y metodos que contiene las funciones de CGP\n",
        "\n",
        "class CGP(object):\n",
        "    def __init__(self, net_info, evaluation_function, offsprings, max_evaluations, mutation_rate):\n",
        "        self.offsprings = offsprings # Cantidad de descendientes\n",
        "        self.pop = [Individual(net_info) for _ in range(1 + self.offsprings)]\n",
        "        self.evaluation_function = evaluation_function\n",
        "        self.max_pool_num = 5 # log2(Tamaño de la imagen = 224) - 2\n",
        "        self.num_gen = 0\n",
        "        self.num_eval = 0\n",
        "        self.modified_evolution(max_evaluations, mutation_rate) # Ejecutamos evolucion modificada\n",
        "\n",
        "    def evaluation(self, pop, eval_flag):\n",
        "        net_lists = []\n",
        "        active_index = np.where(eval_flag)[0]\n",
        "        for i in active_index:\n",
        "            net_lists.append(pop[i].active_net_list())\n",
        "\n",
        "        fp = self.evaluation_function(net_lists)\n",
        "        for i, j in enumerate(active_index):\n",
        "            pop[j].eval = fp[i]\n",
        "        evaluations = np.zeros(len(pop))\n",
        "        for i in range(len(pop)):\n",
        "            evaluations[i] = pop[i].eval\n",
        "\n",
        "        self.num_eval += len(net_lists)\n",
        "        return evaluations\n",
        "\n",
        "    def log_data(self, net_info_type, start_time):\n",
        "        log_list = [self.num_gen, self.num_eval, time.time()-start_time, self.pop[0].eval, self.pop[0].count_active_node()]\n",
        "        if net_info_type == 'active_only':\n",
        "            log_list.append(self.pop[0].active_net_list())\n",
        "        elif net_info_type == 'full':\n",
        "            log_list += self.pop[0].gene.flatten().tolist()\n",
        "        else:\n",
        "            pass\n",
        "        return log_list\n",
        "\n",
        "    def log_data_children(self, net_info_type, start_time, pop):\n",
        "        log_list = [self.num_gen, self.num_eval, time.time()-start_time, pop.eval, pop.count_active_node()]\n",
        "        if net_info_type == 'active_only':\n",
        "            log_list.append(pop.active_net_list())\n",
        "        elif net_info_type == 'full':\n",
        "            log_list += pop.gene.flatten().tolist()\n",
        "        else:\n",
        "            pass\n",
        "        return log_list\n",
        "\n",
        "    def modified_evolution(self, max_eval, mutation_rate):\n",
        "        with open('child.txt', 'w') as fw_c :\n",
        "            writer_c = csv.writer(fw_c, lineterminator='\\n')\n",
        "            start_time = time.time()\n",
        "            eval_flag = np.empty(self.offsprings)\n",
        "            active_num = self.pop[0].count_active_node()\n",
        "            _, pool_num= self.pop[0].check_pool()\n",
        "            self.evaluation([self.pop[0]], np.array([True]))\n",
        "            print(self.log_data(net_info_type='active_only', start_time=start_time))\n",
        "\n",
        "            while self.num_gen < max_eval:\n",
        "                self.num_gen += 1\n",
        "                for i in range(self.offsprings):\n",
        "                    eval_flag[i] = False\n",
        "                    self.pop[i + 1].copy(self.pop[0])\n",
        "                    active_num = self.pop[i + 1].count_active_node()\n",
        "                    _, pool_num= self.pop[i + 1].check_pool()\n",
        "                    while not eval_flag[i] or active_num < self.pop[i + 1].net_info.min_active_num or pool_num > self.max_pool_num:\n",
        "                        self.pop[i + 1].copy(self.pop[0])\n",
        "                        eval_flag[i] = self.pop[i + 1].mutation(mutation_rate)\n",
        "                        active_num = self.pop[i + 1].count_active_node()\n",
        "                        _, pool_num= self.pop[i + 1].check_pool()\n",
        "\n",
        "                evaluations = self.evaluation(self.pop[1:], eval_flag=eval_flag)\n",
        "                best_arg = evaluations.argmax()\n",
        "                f = open('arch_child.txt', 'a')\n",
        "                writer_f = csv.writer(f, lineterminator='\\n')\n",
        "                for c in range(1 + self.offsprings):\n",
        "                    writer_c.writerow(self.log_data_children(net_info_type='full', start_time=start_time, pop=self.pop[c]))\n",
        "                    writer_f.writerow(self.log_data_children(net_info_type='active_only', start_time=start_time, pop=self.pop[c]))\n",
        "                f.close()\n",
        "                if evaluations[best_arg] > self.pop[0].eval:\n",
        "                    self.pop[0].copy(self.pop[best_arg + 1])\n",
        "                else:\n",
        "                    self.pop[0].neutral_mutation(mutation_rate) \n",
        "\n",
        "                print(self.log_data(net_info_type='active_only', start_time=start_time))\n",
        "                fw = open('./log_cgp.txt', 'a')\n",
        "                writer = csv.writer(fw, lineterminator='\\n')\n",
        "                writer.writerow(self.log_data(net_info_type='full', start_time=start_time))\n",
        "                fa = open('arch.txt', 'a')\n",
        "                writer_a = csv.writer(fa, lineterminator='\\n')\n",
        "                writer_a.writerow(self.log_data(net_info_type='active_only', start_time=start_time))\n",
        "                fw.close()\n",
        "                fa.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucion del proceso CGP para la generacion automatica de una arquitectura CNN"
      ],
      "metadata": {
        "id": "zjFIVvl7cmxP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HslqmPYM0Wvt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Cargamos informacion respecto a la estructura CGP que tendra en cuenta a la hora de construir la CNN\n",
        "network_cgp_info = CGP_Structure_Info(rows = 5, \n",
        "                                      columns = 30, \n",
        "                                      level_back = 10, \n",
        "                                      min_active_num = 1, \n",
        "                                      max_active_num = 30)\n",
        "\n",
        "# Guardamos informacion respecto a la estructura CGP\n",
        "with open('network_cgp_info.pickle', mode='wb') as f:\n",
        "  pickle.dump(network_cgp_info, f)\n",
        "\n",
        "# Generamos funcion para el entrenamiento/evaluacion de la CNN autogenerada\n",
        "evaluation_function = CNN_Generated_Evaluation(gpu_number = 1, \n",
        "                                               epoch_number = 20, \n",
        "                                               batch_size = 16, \n",
        "                                               learning_rate = 0.01, \n",
        "                                               number_workers = 4, \n",
        "                                               train_size_percentage = 0.7, \n",
        "                                               train_directory = 'Entrenamiento/')\n",
        "\n",
        "# Se comienza con el proceso CGP llevando a cabo evolucion modificada\n",
        "CGP(net_info = network_cgp_info, \n",
        "    evaluation_function = evaluation_function, \n",
        "    offsprings = 1,\n",
        "    max_evaluations = 1000,\n",
        "    mutation_rate = 0.1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_pytorch_latest_p36",
      "language": "python",
      "name": "conda_pytorch_latest_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}